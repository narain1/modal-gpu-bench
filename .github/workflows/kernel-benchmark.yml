name: Kernel Benchmark

on:
  pull_request:
    branches:
      - main

jobs:
  benchmark:
    # Only run for PRs from narain1 user
    if: github.event.pull_request.user.login == 'narain1'
    runs-on: ubuntu-latest
    
    permissions:
      pull-requests: write
      contents: read
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install Modal CLI
        run: pip install modal
      
      - name: Set Modal token
        run: |
          modal token set --token-id ${{ secrets.MODAL_TOKEN_ID }} --token-secret ${{ secrets.MODAL_TOKEN_SECRET }}
      
      - name: Run kernel benchmarks
        id: benchmark
        run: |
          : > benchmark_results.md
          echo "Kernel Benchmark Results" >> benchmark_results.md
          echo "" >> benchmark_results.md
          echo "PR #${{ github.event.pull_request.number }} | Commit: ${{ github.event.pull_request.head.sha }}" >> benchmark_results.md
          echo "" >> benchmark_results.md

          # Get changed files in kernels directory
          if git rev-parse --is-shallow-repository 2>/dev/null | grep -q true; then
            git fetch --unshallow
          fi
          git fetch origin main
          changed_files=$(git diff --name-only --diff-filter=AM origin/main...HEAD | grep '^kernels/' | grep -E '\.(py|cu)$' || true)

          if [ -z "${changed_files}" ]; then
            echo "No kernel files were modified in this PR." >> benchmark_results.md
            exit 0
          fi

          echo "Kernels Tested" >> benchmark_results.md
          echo "" >> benchmark_results.md

          log_dir="benchmark_logs"
          rm -rf "${log_dir}"
          mkdir -p "${log_dir}"
          GPU_OVERRIDE="H100"

          # Function to run and benchmark a kernel
          run_kernel() {
            local script_path=$1
            local filename=$(basename "$script_path")
            local safe_name=$(echo "$script_path" | tr '/' '_')
            local log_file="${log_dir}/${safe_name}.txt"

            echo "### ${filename}" >> benchmark_results.md
            echo "" >> benchmark_results.md
            echo '```' >> benchmark_results.md

            if modal run modal_run.py --script "${script_path}" --gpu "${GPU_OVERRIDE}" --timeout 2 2>&1 | tee "${log_file}" | tee -a benchmark_results.md; then
              echo '```' >> benchmark_results.md
              echo "${filename} executed successfully on ${GPU_OVERRIDE}" >> benchmark_results.md
            else
              echo '```' >> benchmark_results.md
              echo "${filename} failed to execute on ${GPU_OVERRIDE}" >> benchmark_results.md
            fi
            
            echo "" >> benchmark_results.md
            echo "---" >> benchmark_results.md
            echo "" >> benchmark_results.md
          }

          # Run benchmark for each changed kernel file
          for file in ${changed_files}; do
            if [ -f "${file}" ]; then
              run_kernel "${file}"
            fi
          done

          echo "" >> benchmark_results.md
          echo "GPU: ${GPU_OVERRIDE} | Modal Runtime | Timestamp: $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> benchmark_results.md
      
      - name: Read benchmark results
        id: read_results
        run: |
          encoded=$(python -c "import base64, pathlib; print(base64.b64encode(pathlib.Path('benchmark_results.md').read_bytes()).decode('utf-8'))")
          echo "benchmark_output_base64=${encoded}" >> "$GITHUB_OUTPUT"
      
      - name: Comment PR with results
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const core = require('@actions/core');
            const output = Buffer.from(process.env.BENCHMARK_OUTPUT_BASE64 ?? '', 'base64').toString('utf8');
            const prNumber = context.payload.pull_request?.number;

            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: prNumber,
            });

            const botComment = comments.find(c =>
              c.user?.type === 'Bot' &&
              typeof c.body === 'string' &&
              c.body.includes('Kernel Benchmark Results')
            );

            const commentBody =
              (typeof output === 'string' && output.trim().length > 0)
                ? output
                : 'Kernel Benchmark Results\n\n(no benchmark output produced)';

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: commentBody,
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: prNumber,
                body: commentBody,
              });
            }
        env:
          BENCHMARK_OUTPUT_BASE64: ${{ steps.read_results.outputs.benchmark_output_base64 }}
      
      - name: Ensure benchmark_results.md exists
        run: '[ -f benchmark_results.md ] || touch benchmark_results.md'
      
      - name: Upload benchmark results as artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-results
          path: benchmark_results.md
          retention-days: 30
